{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Exercise: Anomaly detection in high energy physics\n",
    "\n",
    "<img src=\"images/front.png\" alt=\"AD\" width=\"300\" img align=\"right\"/>\n",
    "\n",
    "In this notebook we will demonstrate how to design a tiny autoencoder (AE) that we will use for anomaly detection in particle physics. More specifically, we will demonstrate how we can use autoencoders to select potentially New Physics enhanced proton collision events in a more unbiased way!\n",
    "\n",
    "We will train the autoencoder to learn to compress and decompress data, assuming that for highly anomalous events, the AE will fail.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "As a dataset, we will use the CMS Open data that you have been made familiar with already. Our dataset will be represented as an array of missing transverse energy (MET), up to 4 electrons, up to 4 muons and 10 jets each described by pT, η, φ and particle ID (just from knowing whether it is a muon/electron/jet)--recid 63168 --protocol xrootd. The particles are ordered by pT. If fewer objects are present, the event is zero padded.\n",
    "\n",
    "We will train on a QCD MC dataset (we could also train directly on data), and evaluate the AE performance on a New Physics simulated sample: A Bulk graviton decaying to two vector bosons: G(M=2 TeV) → WW\n",
    "\n",
    "We'll train using background data only and test using both background and the Graviton sample. Let's fetch them! The background data are available [here](https://opendata.cern.ch/record/63168) (recid = 63168) and the signal data [here](https://opendata.cern.ch/record/33703) (recid = 33703). The signal consists of 1,37M events and the background 19,279M events. We will use roughly 500K for each process.\n",
    "\n",
    "We will use the docker client to print all the file names. You can then use this list to concatenate data from all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary ML packages\n",
    "%pip install qkeras==0.9.0 tensorflow==2.11.1 h5py mplhep cernopendata-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "command = [\"cernopendata-client\", \"get-file-locations\", \"--recid\", \"63168\", \"--protocol\", \"xrootd\"]\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "filenames = result.stdout.splitlines()\n",
    "filenames_bkg = np.array(filenames)\n",
    "\n",
    "command = [\"cernopendata-client\", \"get-file-locations\", \"--recid\", \"33703\", \"--protocol\", \"xrootd\"]\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "filenames = result.stdout.splitlines()\n",
    "filenames_sig = np.array(filenames)\n",
    "\n",
    "# Print the NumPy array\n",
    "print(filenames_bkg)\n",
    "print(filenames_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import matplotlib.pylab as plt\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "treename = \"Events\"\n",
    "branch_dict = {\n",
    "                \"Muon\": [\"pt\", \"eta\", \"phi\"],\n",
    "                \"Electron\": [\"pt\", \"eta\", \"phi\"],\n",
    "                \"FatJet\": [\"pt\", \"eta\", \"phi\"],\n",
    "                # \"MET\": [\"pt\", \"phi\"]\n",
    "            }\n",
    "\n",
    "# make list of branches to read from the dictionary above\n",
    "branch_names = []\n",
    "for obj, var in branch_dict.items(): \n",
    "    branch_names += [obj + \"_\" + v for v in var]\n",
    "\n",
    "infiles_sig = filenames_sig[:] # Lets use all the signal files\n",
    "\n",
    "data_sig = uproot.concatenate({fname:\"Events\" for fname in infiles_sig}, \n",
    "                              branch_names, \n",
    "                              how = \"zip\",\n",
    "                              library = \"ak\")\n",
    "\n",
    "# infiles_bkg = [\"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/QCD_Pt-15to7000_TuneCP5_Flat2018_13TeV_pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/130000/2EDCC683-1B4B-614B-BEB7-D80BBC20AD8E.root\",\"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/QCD_Pt-15to7000_TuneCP5_Flat2018_13TeV_pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/19E8D842-3175-1449-AF6C-FD9C69D12724.root\",\"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/QCD_Pt-15to7000_TuneCP5_Flat2018_13TeV_pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/3957434B-7E09-3B4C-8329-FD44D82C7DB7.root\",\"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/QCD_Pt-15to7000_TuneCP5_Flat2018_13TeV_pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/397D1673-167A-CF46-9E79-D7069D9AC359.root\"]\n",
    "infiles_bkg = filenames_bkg[4:8] #Let's use only 4 of the background files, one of the first files seems to not be accessible with xrootd so I take entry 4-8\n",
    "data_bkg = uproot.concatenate({fname:\"Events\" for fname in infiles_bkg}, \n",
    "                              branch_names, \n",
    "                              how = \"zip\",\n",
    "                              library = \"ak\")\n",
    "\n",
    "\n",
    "# # Here is an example of how you can open a single file rith awkward and regex for the branch expression :\n",
    "\n",
    "# file_bkg = uproot.open(infiles_bkg[0])\n",
    "# data_bkg = file_bkg[\"Events\"].arrays(\n",
    "#     filter_name = \"/(Muon|Electron|FatJet|MET)_(pt|eta|phi|sumEt)/\", \n",
    "#     how = \"zip\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have each object grouped in a respective field, and the variables are accesible at the next level (this is a result of using how=\"zip\" when loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sig.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sig.FatJet.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on N signal samples: {len(data_sig)}\")\n",
    "print(f\"Training on N backgr samples: {len(data_bkg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's look at the data and prepare our 1-D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep\n",
    "mplhep.style.use('CMS')\n",
    "\n",
    "for obj in [ 'Muon', 'Electron', 'FatJet']:\n",
    "    plt.figure()#figsize = (8,4))\n",
    "\n",
    "    for label, data in zip([\"bkg\",\"sig\"], [data_bkg, data_sig]):\n",
    "        num = ak.num(data[obj])\n",
    "        plt.hist(num, label = label, bins = range(13), density = True, \n",
    "                 #log = True, \n",
    "                 histtype = \"step\")\n",
    "        \n",
    "    plt.xlabel(f\"N of {obj}\")\n",
    "    plt.legend()\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for obj in [ 'Electron', 'FatJet', 'Muon']:\n",
    "    plt.figure()#figsize = (8,4))\n",
    "\n",
    "    for label, data in zip([\"bkg\",\"sig\"], [data_bkg, data_sig]):\n",
    "        # notice the [:,:1] below -> we slice the array and select no more than the first entry per event\n",
    "        # ak.ravel makes the array flat such that we can fill a histogram\n",
    "        plt.hist(ak.ravel(data[obj].pt[:,:1]), label = label, bins = np.linspace(1, 2000, 101), density = True, \n",
    "                 log = True, \n",
    "                 histtype = \"step\")\n",
    "        \n",
    "    plt.xlabel(f\"{obj} pt\")\n",
    "    plt.legend()\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data to regular array for ML usage\n",
    "\n",
    "Now we need to convert this to something regular that an ML algorithm can use! We will use dedicated functions to ease this conversion from jagged to regular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPadNParr(events, obj, n_pad, fields, cuts = None, name = None, pad_val = 0):\n",
    "    '''\n",
    "    This function filter objects and pads them to a certain length with a given value\n",
    "    '''\n",
    "    \n",
    "    objects = events[obj]\n",
    "    \n",
    "    if not name: name = obj\n",
    "    \n",
    "    pad_arrs = []\n",
    "    var_names = []\n",
    "        \n",
    "    # padding with nones\n",
    "    pad_arr = ak.pad_none(objects, n_pad, clip=True)\n",
    "    \n",
    "    # combining to numpy\n",
    "    for i in range(n_pad):\n",
    "\n",
    "        for var in fields:\n",
    "            pad_arrs += [ak.to_numpy( ak.fill_none(pad_arr[var][:,i], pad_val) )]\n",
    "            var_names.append( \"{}_{}_{}\".format(name, i, var) )\n",
    "            \n",
    "    return np.stack(pad_arrs), var_names\n",
    "\n",
    "def formatData(data, objects, verbosity = 0):\n",
    "    '''\n",
    "    This function concatenates the padded arrays for different objects.\n",
    "    It is controlled via a dictionary as defined above\n",
    "    '''\n",
    "    \n",
    "    # this will be filled by all required objects\n",
    "    dataList = [] \n",
    "    varList = []\n",
    "    \n",
    "    for obj in objects: \n",
    "        print(obj)\n",
    "        dat, names = getPadNParr(data, obj[\"key\"], obj[\"n_obj\"], obj[\"fields\"], obj[\"cuts\"] if \"cuts\" in obj else None, obj[\"name\"] )\n",
    "        dataList.append(dat)\n",
    "        varList += names\n",
    "        \n",
    "    if verbosity > 0:\n",
    "        print(\"The input variables are the following:\")\n",
    "        print(varList)\n",
    "                \n",
    "    # combining and returning (and transforming back so events are along the first axis...)\n",
    "    return np.concatenate(dataList, axis = 0).T, varList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " objects = [\n",
    "    # {\"name\" : \"MET\", \"key\" : \"MET\", \"fields\" : [\"pt\", \"phi\"], \"n_obj\" : 1 },\n",
    "    {\"name\" : \"FatJet\", \"key\" : \"FatJet\", \"fields\" : [\"pt\", \"eta\", \"phi\"], \"n_obj\" : 6},\n",
    "    {\"name\" : \"Electron\", \"key\" : \"Electron\", \"fields\" : [\"pt\", \"eta\", \"phi\"], \"n_obj\" : 4},\n",
    "    {\"name\" : \"Muon\", \"key\" : \"Muon\", \"fields\" : [\"pt\", \"eta\", \"phi\"], \"n_obj\" : 4}\n",
    "]\n",
    "    \n",
    "x_sig, var_names = formatData(data_sig, objects, verbosity = 99) \n",
    "x_bkg, var_names = formatData(data_bkg, objects, verbosity = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the inputs and see whether they make sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,name in enumerate(var_names[:100]):\n",
    "    if \"_0_\" not in name: continue\n",
    "        \n",
    "    plt.figure()\n",
    "    \n",
    "    _ = plt.hist(x_bkg[:,i], bins = 50, log = True, density = True, label = \"Bkg\")\n",
    "    _ = plt.hist(x_sig[:,i], bins = _[1], histtype = \"step\", density = True, label = \"Sig\")\n",
    "    \n",
    "    plt.xlabel(name)\n",
    "    plt.legend()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now that for the non-sum objects we have peaks at 0 as this was the pad_val padding value for filling empty variables. \n",
    "# Final preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's prepare the data!\n",
    "Having very different numerical ranges for the input features, as in this case where $p_T$, $\\eta$ and $\\phi$ have a very diffent mean, makes it difficult for the network to converge. We will therefore standardize the $p_T$.  Also, when there is no lepton present in the event, the values will be zero padded. That means there is an unnatural number of zero entries in the $p_T$. For a real use case, the loss should be rewritten as to not take zero entries into account. Ideally we would also deal with the periodicity of phi etc. Details on the best procedure for preprccesing can be found in [this paper](https://arxiv.org/abs/2108.03986). In the interest of time, we'll only do the $p_T$ scaling.\n",
    "\n",
    "In addition, we split the training data into train/test sets and save them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "_ = scaler.fit(x_bkg)\n",
    "x_bkg_scaled = scaler.transform(x_bkg)\n",
    "x_sig_scaled = scaler.transform(x_sig)\n",
    "    \n",
    "        \n",
    "# define training, test and validation datasets\n",
    "X_train, X_test = train_test_split(x_bkg_scaled, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"Training data shape = \",X_train.shape)    \n",
    "with h5py.File('bkg_dataset.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('X_train', data = X_train)\n",
    "    h5f.create_dataset('X_test', data = X_test)\n",
    "    \n",
    "with h5py.File('signal_dataset.h5', 'w') as h5f2:\n",
    "    h5f2.create_dataset('Data', data = x_sig_scaled)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You  now have two new files in your reposity, `bkg_dataset.h5` and `signal_dataset.h5` which contains your train/test/val data to train the autoencoder, as well as a test data to check your performance on a New Physics signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('bkg_dataset.h5', 'r') as file:\n",
    "    X_train = np.array(file['X_train'])\n",
    "    X_test = np.array(file['X_test'])\n",
    "    \n",
    "with h5py.File('signal_dataset.h5', 'r') as file:\n",
    "    signal_test_data = np.array(file['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\" Training (#samples,#features):\", X_train.shape)\n",
    "print(\" Testing  (#samples,#features):\", signal_test_data.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(16,10))\n",
    "fig.suptitle('Kinematic distributions in bkg vs signal')\n",
    "\n",
    "axs[0].hist(X_train[:,0],bins=100,label=r'Background',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[0].hist(signal_test_data[:,0 ],bins=100,label=r'Signal',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[0].semilogy()\n",
    "axs[0].set(xlabel=u'Leading jet $p_{T}$ ( Norm. GeV)', ylabel='A.U')\n",
    "axs[0].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "\n",
    "axs[1].hist(X_train[:,18],bins=100,label=r'Background',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[1].hist(signal_test_data[:,18],bins=100,label=r'Signal',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[1].set(xlabel=u'Leading electron $p_{T}$ (Norm. GeV)', ylabel='A.U')\n",
    "axs[1].semilogy()\n",
    "axs[1].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "print(\"Example data (1,57):\",signal_test_data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the autoencoder\n",
    "\n",
    "Now, let's define an autoencoder to learn to reconstruct the training data after compressing it through a bottleneck, then decompressing it again.\n",
    "\n",
    "<img src=\"images/ae.png\" alt=\"The autoencoder\" width=\"800\" img align=\"center\"/>\n",
    "\n",
    "For that, we need a stack of dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "latent_dim = 3\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape,))\n",
    "#x = BatchNormalization()(inputArray) #Only use this if you're not standardizing the pT\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(inputArray)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True #If you have a pre-trained model you can set this to false and load the other instead\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "if train:\n",
    "    history = autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "              validation_split=0.2,\n",
    "              callbacks=callbacks)\n",
    "    # Save the model\n",
    "    autoencoder.save('baseline_ae.h5')\n",
    "    autoencoder.save_weights('baseline_ae.weights.h5')\n",
    "    \n",
    "else:\n",
    "    autoencoder = tf.keras.models.load_model('baseline_ae.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model performance\n",
    "\n",
    "Remember that the key metric we use for anomaly detection is the mean-squared-error: If the error is high, the data is more likely to be anomalous, and if the error is low, the data is similar to the training data (which in our case is SM events). We therefore first need to run `model.predict()` in order to get the AE reconstructed output, both for our vanilla SM test data, and for our new leptoquark signal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_prediction = autoencoder.predict(X_test)\n",
    "signal_prediction = autoencoder.predict(signal_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the network reconstructs the transverse momentum of the leptons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(16,8))\n",
    "fig.suptitle('Real data versus reconsructed data')\n",
    "axs[0].hist(X_test[:,0],bins=100,label=r'Real data',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[0].hist(bkg_prediction[:,0],bins=100,label=r'Predicted',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[0].semilogy()\n",
    "axs[0].set(xlabel=u'Leading fatjet $p_{T}$ ( Norm. GeV)', ylabel='A.U')\n",
    "axs[0].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "\n",
    "axs[1].hist(X_test[:,18],bins=100,label=r'Real data',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[1].hist(bkg_prediction[:,18],bins=100,label=r'Predicted',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[1].set(xlabel=u'Leading electron $p_{T}$ (Norm. GeV)', ylabel='A.U')\n",
    "axs[1].semilogy()\n",
    "axs[1].legend(loc='best',frameon=False, ncol=1,fontsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructed $p_T$ has an unphysical tail below zero. For a real usecase, we would force the output to be positive for that column, as well as treat the zero-padded cases differently in the loss.\n",
    "To keep it short, we won't do that now, but you can read more about it [here](https://arxiv.org/abs/2108.03986)\n",
    "\n",
    "We then need to compute the mean-square-error, which will be our final discriminating variable. This you would need to write custom if applying the changes we mention above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(true, prediction):\n",
    "    loss = tf.reduce_mean(tf.math.square(true - prediction),axis=-1)\n",
    "    return loss\n",
    "\n",
    "# compute loss value of input data versus AE reconstructed data\n",
    "mse_sm = mse_loss(X_test, bkg_prediction.astype(np.float32)).numpy()\n",
    "mse_bsm = mse_loss(signal_test_data,signal_prediction.astype(np.float32)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at our discriminant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size=100\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "n, bins, patches =plt.hist(mse_bsm, bins=bin_size, label=\"BSM\", density = True, histtype='step', fill=False, edgecolor='orchid', linewidth=1.5,range=[0,200])\n",
    "n_, bins_, patches_ =plt.hist(mse_sm, bins=bin_size, label=\"SM Background\", density = True, histtype='step', fill=False, edgecolor='green', linewidth=1.5,range=[0,200])\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Mean-squared-error\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some discrimination power if we cut at very high values of the MSE! Let's look at a ROC curve to make it easier to vizualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "target_background = np.zeros(mse_sm.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "trueVal = np.concatenate((np.ones(mse_bsm.shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "predVal_loss = np.concatenate((mse_bsm, mse_sm))\n",
    "\n",
    "fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "plt.plot(fpr_loss, tpr_loss, \"-\", label='BSM (auc = %.1f%%)'%(auc_loss*100.), linewidth=1.5, color = \"orchid\")\n",
    "    \n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='green', linestyle='dashed', linewidth=2) # threshold value for measuring anomaly detection efficiency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can obviously be further improved, but I leave that up to you :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compression\n",
    "<img src=\"images/nmi_qkeras_hls4ml.jpeg\" alt=\"hls4ml and qkeras\" width=\"200\" img align=\"left\"/>\n",
    "\n",
    "Now, there is absolutely no way anyone would let you deploy this model on an FPGA in the trigger. It will use far too many resources! Luckily, as we discussed in the lecture, there are some cheap tricks you can perform to compress the model. These are pruning and quantization-aware-training and both are very easily implemented. Let's have a look.\n",
    "\n",
    "To quantize the model during training, such that the network will get the opportunity to adapt to the narrower bitwidth we use the library [QKeras](https://www.nature.com/articles/s42256-021-00356-5.epdf?sharing_token=A6MQVmmncHNyCtDUXzrqtNRgN0jAjWel9jnR3ZoTv0N3uekY-CrHD1aJ9BTeJNRfQ1EhZ9jJIhgZjfrQxrmxMLMZ4eGzSeru7-ASFE-Xt3NVE6yorlffwUN0muAm1auU2I6-5ug4bOLCRYvA0mp-iT-OdPsrBYeH0IHRYx0t3wc%3D), developed in a joint effort between CERN and Google.\n",
    "\n",
    "# Quantization aware training with QKeras\n",
    "\n",
    "Quantization is a powerful way to reduce model memory and resource consumption. In this tutorial, we will use the libary QKeras to perform quantization aware training (QAT).\n",
    "\n",
    "In contrast to in Keras, where models are trained using floating point precision, QKeras quantizes each of the model weights and activation functions during training, allowing the network to adapt to the numerical precision that will eventually be used on hardware.\n",
    "\n",
    "During the forward pass of the network, each floating point weight is put into one of $2^{bitwidth}$ buckets. Which one it goes into is defined through rounding and clipping schemes.\n",
    "\n",
    "Below you can see an example of a tensor with a (symmetric) dynamic range of $x_{f}$ $[-amax, amax]$ mapped through quantization to a an 8 bit integer, $2^8=256$ discrete values in the interval $[-128, 127]$ (32-bit floating-point can represent ~4B numbers in the interval $[-3.4e38, 3.40e38]$).\n",
    "\n",
    "<img src=\"https://gitlab.cern.ch/fastmachinelearning/cms_mlatl1t_tutorial/-/raw/master/part2/images/8-bit-signed-integer-quantization.png?ref_type=heads\" width=\"800\"/>\n",
    "\n",
    "Quantization of floating point numbers can be achieved using the quantization operation\n",
    "\n",
    "$$x_{q} = Clip(Round(x_{f}/scale))$$\n",
    "\n",
    "where $x_{q}$ is the quantized digit and $x_{f}$ is the floating point digit. $Round$ is a function that applies some rounding scheme to each number and $Clip$ is a function that clips outliers that fall outside the $[-128, 127]$ interval. The $scale$ parameter is obtained by dividing the float-point dynamic-range into 256 equal parts.\n",
    "\n",
    "On FPGA, we do not use int8 quantization, but fixed-point quantization, bu the idea is similar. Fixed-point representation is a way to express fractions with integers and offers more control over precision and range. We can split the $W$-bits making up an integer (in our case $W=8$) to represent the integer part of a number and the fractional part of the number. We usually reserve 1-bit representing the sign of the digit. The radix splits the remaining $W-1$ bits to $I$ most significant bits representing the integer value and $F$ least significant bits representing the fraction. We write this as $<W,I>$, where $F=W-1-I$.  Here is an example for an unsigned $<8,3>$:\n",
    "\n",
    "<img src=\"https://gitlab.cern.ch/fastmachinelearning/cms_mlatl1t_tutorial/-/raw/master/part2/images/fixedpoint.png?ref_type=heads\" width=\"400\"/>\n",
    "\n",
    "\n",
    "This fixed point number corresponds to $2^4\\cdot0+2^3\\cdot0+2^2\\cdot0+2^1\\cdot1+2^0\\cdot0+2^{-1}\\cdot1+2^{-2}\\cdot1+2^{-3}\\cdot0=2.75$.\n",
    "\n",
    "The choice of $I$ and $F$ has to be derived as a trade-off between representation range and precision, where $I$ controls the range and $F$ the precision.\n",
    "\n",
    "In the following we will use a bitwidth of 8 and 0 integer bits. Not considering the sign bit, this means that the smallest number you can represent (the precision) and the largest number (the range) is:\n",
    "\n",
    "$$ \\rm{Precision}= \\frac{1}{2^{F}}= \\frac{1}{2^8} = 0.00390625$$\n",
    "$$\\rm{Range}= [-2^0,-2^0-1]=[-1,0] $$\n",
    "With zero integer bits the largest number you can represent is just below (but not including) 1. For weights in deep neural networks, being constrained to be less than 1 is often a reasonable assumtion.\n",
    "\n",
    "\n",
    "\n",
    "What QKeras (and other QAT libraries) do, is to include the quantization error during the training, in the following way:\n",
    "- \"Fake quantize\" the floating-point weights and activations during the forward pass: quantize the weights and use them for the layer operations\n",
    "- Immediately un-quantize the parameters so the rest of the computations take place in floating-point\n",
    "- During the backward pass, the gradient of the weights is used to update the floating point weight\n",
    "- The quantization operation gradient (zero or undefined) is handled by passing the gradient through as is (\"straight through estimator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating to a QKeras QAT model\n",
    "There are two ways to translate this into a QKeras model that can be trained quantization aware, lets first do it manually:\n",
    "\n",
    "### Manual QKeras model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "latent_dim = 3\n",
    "\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape,))\n",
    "# x = BatchNormalization()(inputArray)\n",
    "x = QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,3,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,3,1, alpha=1.0)')(inputArray)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "x = QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,3,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,3,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "encoder = QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(16,6,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(16,6,1, alpha=1.0)', name='bottleneck')(x)\n",
    "\n",
    "#decoder\n",
    "x = QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,3,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,3,1, alpha=1.0)')(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "x = QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,3,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,3,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "decoder = QDense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(16,6,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(16,6,1, alpha=1.0)')(x)\n",
    "\n",
    "#create autoencoder\n",
    "q_autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "q_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait! What is going on here?\n",
    "The magic happens in ```quantized_bits``` (see implementation [here](https://github.com/google/qkeras/blob/master/qkeras/quantizers.py#L1245)), where the parameters are the following:\n",
    "- ```bits```: The bitwidth, allowing you to have $2^{bits}$ unique values of each weight parameter\n",
    "- ```integers```: How many are integer bits, in this case zero. All 8 bits are used to represent the fractional part of the weight parameter, with no bits dedicated to representing whole numbers. This forces the value to be between -1 and 1. For DNNs this can be useful because the focus is entirely on the precision of the fraction rather than the magnitude of the number. Question: Would this also work on the output node if your algorithm is a regression of the jet mass?\n",
    "- ```symmetric```: should the values be symmetric around 0? In this case it doesnt have to be.\n",
    "- ```alpha```: with $2^W$ unique values available, we could let them go from $[-2^W, 2^W-1]$ like above, but we can also let them go from $[-2^W*\\alpha, (2^W-1)*\\alpha]$. ```alpha``` is a scaling of the weights. Enabling this often leads to improved performance, but it doesnt talk so nicely to hls4ml, so we recommend leaving it at 1 (or get ready for having to debug)\n",
    "\n",
    "Having added this, QKeras will automatically apply fake quantization for us during the forward pass, accounting for the quantization error and returning a network that is optimized for the precision you plan on using in hardware.\n",
    "\n",
    "Another thing to notice is that we leave the sigmoid and the final output logit unquantized. This is because this is were we want the values to be very accurate, and it is not going to save us a lot of resources quantizing it.\n",
    "\n",
    "\n",
    "### Automatic model quantization through config\n",
    " You can also set the quantization for the full model using a model configuration. Sometimes this can be sater if you're using the same quantizer for all layers of the same type. You don't have to use this for this tutorial, we already have a model, but we will leave it here as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoQuant = False\n",
    "\n",
    "if autoQuant:\n",
    "    config = {\n",
    "      \"QDense\": {\n",
    "          \"kernel_quantizer\": \"quantized_bits(bits=8, integer=0, symmetric=0, alpha=1)\",\n",
    "          \"bias_quantizer\": \"quantized_bits(bits=8, integer=0, symmetric=0, alpha=1)\",\n",
    "      },\n",
    "      \"QActivation\": { \"relu\": \"quantized_relu(8)\" }\n",
    "    }\n",
    "    from qkeras.utils import model_quantize\n",
    "\n",
    "    qmodel = model_quantize(autoencoder, config, 4, transfer_weights=True)\n",
    "\n",
    "    for layer in qmodel.layers:\n",
    "        if hasattr(layer, \"kernel_quantizer\"):\n",
    "            print(layer.name, \"kernel:\", str(layer.kernel_quantizer_internal), \"bias:\", str(layer.bias_quantizer_internal))\n",
    "        elif hasattr(layer, \"quantizer\"):\n",
    "            print(layer.name, \"quantizer:\", str(layer.quantizer))\n",
    "\n",
    "    print()\n",
    "    qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But be careful that activation functions like softmax/sigmoid and perhaps logit layers you want to keep at full presision doesn't get quantized!\n",
    "\n",
    "## But how many bits?\n",
    "\n",
    "So now we know how to quantize our models, but how do we know wich precision to choose?\n",
    "Finding the best number of bits and integer bits to use is non-trivial, and there are two ways we recommend:\n",
    "- The easiest strategy is to scan over the possible bit widths from binary up to some maximum value and choose the smallest one that still has acceptable accuracy, and this is what we often do. \n",
    "Code for how to do this can be found [here](https://github.com/thesps/keras-training/blob/qkeras/train/train_scan_models.py#L16), and is illustrated below.\n",
    "For binary and ternary quantization, we use the special ```binary(alpha=1.0)(x)``` and ```ternary(alpha=1.0)(x)``` quantizers. \n",
    "\n",
    "<img src=\"https://gitlab.cern.ch/fastmachinelearning/cms_mlatl1t_tutorial/-/raw/master/part2/images/quant_scan.png?ref_type=heads\" width=\"400\"/>\n",
    "\n",
    "- Another thing you can do is to use our library for automatic quantization, [AutoQKeras](https://github.com/google/qkeras/blob/master/notebook/AutoQKeras.ipynb), to find the optimal quantization for each layer. This runs hyperparameter optimisation over quantizers/nodes/filters simultenously. An example can be found at the end of [this notebook](https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part6_cnns.ipynb) \"Bonus exercise: Automatic quantization with AutoQKeras\".\n",
    "\n",
    "## Pruning\n",
    "\n",
    "Besides reducing the numerical precision of all the weights, biases and activations, I also want to remove neurons and synapses that do not contribute much to the network overall decision. We do that by pruning, let's remove 50\\% of the weights (spasity=0.5):\n",
    "Let's make sure to skip our latent dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "def pruneFunction(layer):\n",
    "    pruning_params = {'pruning_schedule': pruning_schedule.PolynomialDecay(initial_sparsity=0.05,final_sparsity=0.50, begin_step=0, end_step=100, frequency=100)}\n",
    "    if isinstance(layer, tf.keras.layers.Dense) and layer.name!='bottleneck':\n",
    "      return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)  \n",
    "    return layer\n",
    "\n",
    "# q_autoencoder.load_weights('baseline_ae.weights.h5')\n",
    "qp_autoencoder = tf.keras.models.clone_model( q_autoencoder, clone_function=pruneFunction)\n",
    "qp_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=False))\n",
    "callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "if train:\n",
    "        \n",
    "    history = qp_autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)\n",
    "    # Save the model\n",
    "    qp_autoencoder = strip_pruning(qp_autoencoder)\n",
    "    qp_autoencoder.save('qkeras_ae.h5')\n",
    "    \n",
    "else:\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    co = {}; _add_supported_quantized_objects(co)\n",
    "\n",
    "    qp_autoencoder = tf.keras.models.load_model('qkeras_ae.h5',custom_objects=co)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check that the pruning worked and 50% of the weights are indeed 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWeights(model):\n",
    "\n",
    "    allWeightsByLayer = {}\n",
    "    for layer in model.layers:\n",
    "        if (layer._name).find(\"batch\")!=-1 or len(layer.get_weights())<1:\n",
    "            continue \n",
    "        weights=layer.weights[0].numpy().flatten()  \n",
    "        allWeightsByLayer[layer._name] = weights\n",
    "        print('Layer {}: % of zeros = {}'.format(layer._name,np.sum(weights==0)/np.size(weights)))\n",
    "\n",
    "    labelsW = []\n",
    "    histosW = []\n",
    "\n",
    "    for key in reversed(sorted(allWeightsByLayer.keys())):\n",
    "        labelsW.append(key)\n",
    "        histosW.append(allWeightsByLayer[key])\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    bins = np.linspace(-1.5, 1.5, 50)\n",
    "    plt.hist(histosW,bins,histtype='stepfilled',stacked=True,label=labelsW, edgecolor='black')\n",
    "    plt.legend(frameon=False,loc='upper left')\n",
    "    plt.ylabel('Number of Weights')\n",
    "    plt.xlabel('Weights')\n",
    "    plt.figtext(0.2, 0.38,model._name, wrap=True, horizontalalignment='left',verticalalignment='center')\n",
    "    \n",
    "doWeights(autoencoder) \n",
    "doWeights(qp_autoencoder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, a huge spike at zero for the pruned model! Let's compare the performance to the floating point precision, unpruned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_prediction = qp_autoencoder.predict(X_test)\n",
    "signal_prediction = qp_autoencoder.predict(signal_test_data)\n",
    "\n",
    "# compute loss value of input data versus AE reconstructed data\n",
    "q_mse_sm = mse_loss(X_test, bkg_prediction.astype(np.float32)).numpy()\n",
    "q_mse_bsm = mse_loss(signal_test_data,signal_prediction.astype(np.float32)).numpy()\n",
    "\n",
    "target_background = np.zeros(q_mse_sm.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "trueVal = np.concatenate((np.ones(q_mse_bsm.shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "predVal_loss = np.concatenate((q_mse_bsm, q_mse_sm))\n",
    "\n",
    "q_fpr_loss, q_tpr_loss, q_threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "q_auc_loss = auc(q_fpr_loss, q_tpr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "    \n",
    "plt.plot(fpr_loss, tpr_loss, \"-\", label='Baseline (auc = %.1f%%)'%(auc_loss*100.), linewidth=1.5, color = \"orchid\")\n",
    "plt.plot(q_fpr_loss, q_tpr_loss, \"-\", label='Quantized+Pruned (auc = %.1f%%)'%(q_auc_loss*100.), linewidth=1.5, color = \"green\")\n",
    "\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.ylim(0.001,1)\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='orange', linestyle='dashed', linewidth=2) # threshold value for measuring anomaly detection efficiency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to appreciate what an absolutely fantastic result this is! We can reduce the numerical precision of the model from floating point 32 representation to 8-bit fixed point precision, remove 50% of the weights *and still get the same performance*!! This is a crucial, and absolutely neccessary, part of making deep neural networks in particle detector data aquisition systems a reality (also, note that AUC is not a good metric for trigger applications, where you typically operate at very low false positive rates of 10E-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems despite having reduced the numerical precision of the model and the input, as well as removing 50% of the model weights, we're doing pretty good! This can be tuned to get even better, by carefully adjusting the input precision and the model precision, especially increaseing the precision of the logit layer.\n",
    "\n",
    "# Generating firmware with\n",
    "\n",
    "<img src=\"https://gitlab.cern.ch/fastmachinelearning/cms_mlatl1t_tutorial/-/raw/master/part2/images/hls4ml_logo.png?ref_type=heads\" width=\"400\"/>\n",
    "\n",
    "Time to translate this model into HLS (which we will integrate in the emulator) and use to generate the vhdl to be integrated in the trigger firmware. We will use the Python library hls4ml for that ([here](https://github.com/fastmachinelearning/hls4ml-tutorial/tree/main) is the hls4ml tutorial).\n",
    "hls4ml seamlessly talks to QKeras, making our jobs way easier for us, but there is still some work for us to do to make sure we get good hardware model accuracy. Lets start!\n",
    "There are a few things I already know in advance and would like my model to include:\n",
    "- Be execuded fully parallel (=unrolled) to reach the lowest possible latency. We set the ReuseFactor=1 and Strategy=Latency\n",
    "- The correct input precision\n",
    "- The correct model output (that's something you have to figure out yourself!)\n",
    "- Use \"correct\" precisions to make sure the hardware model performs the same as the software one. QKeras handles weights/biases and activation functions for us, but there are a few other parameters that need to be set by hand\n",
    "\n",
    "For the final point, have a look at the following diagram:\n",
    "\n",
    "<img src=\"https://gitlab.cern.ch/fastmachinelearning/cms_mlatl1t_tutorial/-/raw/master/part2/images/hls4ml_precisions.png?ref_type=heads\" width=\"400\"/>\n",
    "\n",
    "Whereas the $weight$ and $bias$ is set to its optimal value from the QKeras model, the accumulator $accum$ and $result$ is set to some default value that might not be optimal for a given model and might need tuning. Let's do a first attemt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hls4ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "\n",
    "def print_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key), end='')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_dict(value, indent + 1)\n",
    "        else:\n",
    "            print(':' + ' ' * (20 - len(key) - 2 * indent) + str(value))\n",
    "            \n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(qp_autoencoder, granularity='name')\n",
    "config[\"Model\"][\"Strategy\"] = \"Latency\"\n",
    "config[\"Model\"][\"ReuseFactor\"] = 1\n",
    "\n",
    "inputPrecision = \"ap_fixed<16,7,AP_RND,AP_SAT>\" #The input also will be quantized! quantized_bits(16,6) becomes \"ap_fixed<16,7>. Adding one bit for the sign, different definitions QKeras/Vivado\n",
    "for layer in qp_autoencoder.layers:\n",
    "    if layer.__class__.__name__ in [\"InputLayer\"]:\n",
    "        config[\"LayerName\"][layer.name][\"Precision\"] = inputPrecision\n",
    "config[\"LayerName\"][\"q_dense_4\"][\"Precision\"][\"result\"] = \"ap_fixed<16,7,AP_RND,AP_SAT>\"       \n",
    "\n",
    "# If the logit layer is a \"normal\" Keras kayer and has not been quantized during the training, \n",
    "# we need to be careful setting the accuracy. This can be done in the following way:\n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"weight\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"bias\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"accum\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "# config[\"LayerName\"][\"logits\"][\"Precision\"][\"result\"] = \"ap_fixed<13,2,AP_RND,AP_SAT>\" \n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(qp_autoencoder, \n",
    "                                                       hls_config=config, \n",
    "                                                       io_type='io_parallel', #other option is io_stream\n",
    "                                                       output_dir='L1TMLDemo_v1',\n",
    "                                                       project_name='L1TMLDemo_v1', \n",
    "                                                       part='xcu250-figd2104-2L-e', #Target FPGA, ideally you would use VU9P and VU13P that we use in L1T but they are not installed at lxplus, this one is close enought for this\n",
    "                                                       clock_period=2.5, # Target frequency 1/2.5ns = 400 MHz\n",
    "#                                                        input_data_tb='qX_test.npy', # For co-simulation\n",
    "#                                                        output_data_tb='qy_test.npy',# For co-simulation\n",
    ")\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, what does our newly created model look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the precision is what we set it to be in QKeras as well as what we set manually in the config. One thing to note is the different definitions used in QKeras and in ap_fixed:\n",
    "- ```quantized_bits(8,0) -> ap_fixed<8,1>```\n",
    "- ```quantized_relu(8,0) -> ap_ufixed<8,0>```\n",
    "Also you can see that the defualt value for result/accu is set to $16,6$. This can also be tuned to more optimal values.\n",
    "\n",
    "## Validate the firmware model accuracy\n",
    "\n",
    "#et's also run predict on the C++ implementation of our model and make sure it's the ~same as for the QKeras model.\n",
    "This is very slow for the C++ implementation of our model, but we need a lot of statistics to probe the low rate region.\n",
    "\n",
    "### <span style=\"color:green\">Execute the next cell, then grab a quick coffee while its running (<5 minutes)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_bkg_prediction = hls_model.predict(np.ascontiguousarray(X_test))\n",
    "hls_signal_prediction = hls_model.predict(np.ascontiguousarray(signal_test_data))\n",
    "\n",
    "# compute loss value of input data versus AE reconstructed data\n",
    "hls_mse_sm = mse_loss(X_test, hls_bkg_prediction.astype(np.float32)).numpy()\n",
    "hls_mse_bsm = mse_loss(signal_test_data,hls_signal_prediction.astype(np.float32)).numpy()\n",
    "\n",
    "target_background = np.zeros(hls_mse_sm.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "trueVal = np.concatenate((np.ones(hls_mse_bsm.shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "predVal_loss = np.concatenate((hls_mse_bsm, hls_mse_sm))\n",
    "\n",
    "hls_fpr_loss, hls_tpr_loss, hls_threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "hls_auc_loss = auc(hls_fpr_loss, hls_tpr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print and compare some of the predictions:\n",
    "print(f\"Truth input:\\n {X_test[0]}\\n\")\n",
    "print(f\"Qkeras reconstruction:\\n {bkg_prediction[0]}\\n\")\n",
    "print(f\"HLS reconstruction:\\n {hls_bkg_prediction[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot it!\n",
    "plt.figure()\n",
    "    \n",
    "plt.plot(fpr_loss, tpr_loss, \"-\", label='Baseline (auc = %.1f%%)'%(auc_loss*100.), linewidth=1.5, color = \"orchid\")\n",
    "plt.plot(q_fpr_loss, q_tpr_loss, \"-\", label='Quantized+Pruned (auc = %.1f%%)'%(q_auc_loss*100.), linewidth=1.5, color = \"green\")\n",
    "plt.plot(hls_fpr_loss, hls_tpr_loss, \"--\", label='HLS (auc = %.1f%%)'%(hls_auc_loss*100.), linewidth=1.5, color = \"red\")\n",
    "\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.ylim(0.001,1)\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! That was easier than expected. If you see the accuracies differing significantly, it's a good idea to look into accumulator and reult precisions. Also with tools like $Trace$ and $Profiling$ that you can learn from in the [official hls4ml tutorial](https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part2_advanced_config.ipynb) can be helpful! In this case, it doesnt seem like it's necessary. \n",
    "\n",
    "## Synthesise!\n",
    "\n",
    "Now let's build it! Lets run C-synthesis (C++ to register-transfer level) and Vivado logic synthesis (gate level representation). We will not do co-simulation (send test vectors, do an exhaustive functional test of the implemented logic), but this can be a good idea if you are using CNNs and the $io_stream$ io. \n",
    "\n",
    "Let's run!\n",
    "\n",
    "### <span style=\"color:green\">Sadly we won't get any further for this tutorial. Idid not manage to include a Vitis licenses and software for this tutorial. But you can work through the [hls4ml tutorial](https://github.com/fastmachinelearning/hls4ml-tutorial/tree/main), where there is a container with Vivado available!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. This is not such a great anomaly detection algorithm. Can you improve it? I would start adding by adding other input features\n",
    "2. Making an event-level anomaly detection algorithm is useful to be sensitive to many different BSM final states. In this exercise, see if you can rather make an anomalous jet detection algorithm (hint: you have access to other features of the FatJet than the four-momentum!)\n",
    "3. Make your own algorithm using the tools you have learnt here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint, if you want to include MET or HT they won't become \"objects\" like the electron/muon/jets. You could also not \"zip\" when loading, and then do the zip'ing manually:\n",
    "```\n",
    "# Lets convert to correct formatting ao that all objects are zipped together as Momentum4D vectors\n",
    "new_arr_sig = {}\n",
    "for obj, brs in branch_dict.items():\n",
    "    new_arr_sig[obj] = ak.zip({ br : data_sig[obj + \"_\" + br] for br in brs} )\n",
    "      \n",
    "  \n",
    "# making sure it is a Momentum4D object like the rest\n",
    "final_arr_sig = {}\n",
    "for key, value in new_arr_sig.items(): final_arr_sig[key] = ak.Array( value, with_name = \"Momentum4D\" )\n",
    "\n",
    "len(ak.Array(final_arr_sig))\n",
    "\n",
    "new_arr_bkg = {}\n",
    "for obj, brs in branch_dict.items():\n",
    "    new_arr_bkg[obj] = ak.zip({ br : data_bkg[obj + \"_\" + br] for br in brs} )\n",
    "      \n",
    "  \n",
    "# making sure it is a Momentum4D object like the rest\n",
    "final_arr_bkg = {}\n",
    "for key, value in new_arr_bkg.items(): final_arr_bkg[key] = ak.Array( value, with_name = \"Momentum4D\" )\n",
    "\n",
    "len(ak.Array(final_arr_sig))\n",
    "\n",
    "data_sig = ak.Array(final_arr_sig)\n",
    "data_bkg = ak.Array(final_arr_bkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
